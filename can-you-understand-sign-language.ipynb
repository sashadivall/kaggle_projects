{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5337,"sourceType":"datasetVersion","datasetId":3258}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-10T13:52:05.730057Z","iopub.execute_input":"2024-07-10T13:52:05.731095Z","iopub.status.idle":"2024-07-10T13:52:06.931462Z","shell.execute_reply.started":"2024-07-10T13:52:05.731053Z","shell.execute_reply":"2024-07-10T13:52:06.930155Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sign-language-mnist/sign_mnist_test.csv\n/kaggle/input/sign-language-mnist/amer_sign2.png\n/kaggle/input/sign-language-mnist/amer_sign3.png\n/kaggle/input/sign-language-mnist/sign_mnist_train.csv\n/kaggle/input/sign-language-mnist/american_sign_language.PNG\n/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\n/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets,models,transforms\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nfrom PIL import Image\nfrom tempfile import TemporaryDirectory\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_image","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:46.848361Z","iopub.execute_input":"2024-07-10T14:52:46.848864Z","iopub.status.idle":"2024-07-10T14:52:46.856616Z","shell.execute_reply.started":"2024-07-10T14:52:46.848825Z","shell.execute_reply":"2024-07-10T14:52:46.855200Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')\ntest_data = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')\n# Extract labels and pixel data\ntrain_labels = train_data['label'].values\ntest_labels = test_data['label'].values\ntrain_pixels = train_data.iloc[:, 1:].values.astype('float32')\ntest_pixels = test_data.iloc[:, 1:].values.astype('float32')\n\n# Normalize pixel values to range [0,1]\ntrain_pixels /= 255.0\ntest_pixels /= 255.0\n\n# Reshape the images (each image is 28x28, grayscale)\ntrain_pixels = train_pixels.reshape(-1, 1, 28, 28)\ntest_pixels = test_pixels.reshape(-1,1,28,28)\n\n# Convert NumPy arrays to PyTorch tensors \ntrain_pixels_tensor = torch.tensor(train_pixels)\ntrain_labels_tensor = torch.tensor(train_labels)\n\ntest_pixels_tensor = torch.tensor(test_pixels)\ntest_labels_tensor = torch.tensor(test_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:46:53.634966Z","iopub.execute_input":"2024-07-10T14:46:53.636265Z","iopub.status.idle":"2024-07-10T14:46:56.824378Z","shell.execute_reply.started":"2024-07-10T14:46:53.636218Z","shell.execute_reply":"2024-07-10T14:46:56.823335Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Defining a custom dataset class\nclass SignLanguageDataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n        return image, label\n    \n# Create datasets and data loaders\ntrain_dataset = SignLanguageDataset(train_pixels_tensor, train_labels_tensor)\ntest_dataset = SignLanguageDataset(test_pixels_tensor, test_labels_tensor)\n\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:49:58.275960Z","iopub.execute_input":"2024-07-10T14:49:58.276601Z","iopub.status.idle":"2024-07-10T14:49:58.286795Z","shell.execute_reply.started":"2024-07-10T14:49:58.276566Z","shell.execute_reply":"2024-07-10T14:49:58.285181Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Defining the Neural Network Model\nclass SignLanguageNet(nn.Module):\n    def __init__(self):\n        super(SignLanguageNet, self).__init__()\n        self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1)\n        self.conv2 = nn.Conv2d(16,32, kernel_size=3,stride=1,padding=1)\n        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128,25)\n        \n    def forward(self,x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = x.view(-1, 32 * 7 * 7)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Create an instance of the model\nmodel = SignLanguageNet()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:02:23.325440Z","iopub.execute_input":"2024-07-10T15:02:23.325905Z","iopub.status.idle":"2024-07-10T15:02:23.341047Z","shell.execute_reply.started":"2024-07-10T15:02:23.325853Z","shell.execute_reply":"2024-07-10T15:02:23.339503Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Training the model\n\n# define the loss function and the optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nnum_epochs = 25\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n    epoch_loss = running_loss / len(train_loader.dataset)\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n    \nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:03:59.403771Z","iopub.execute_input":"2024-07-10T15:03:59.404824Z","iopub.status.idle":"2024-07-10T15:07:26.333125Z","shell.execute_reply.started":"2024-07-10T15:03:59.404782Z","shell.execute_reply":"2024-07-10T15:07:26.331971Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1/25, Loss: 0.4485\nEpoch 2/25, Loss: 0.3570\nEpoch 3/25, Loss: 0.2896\nEpoch 4/25, Loss: 0.2149\nEpoch 5/25, Loss: 0.1701\nEpoch 6/25, Loss: 0.1364\nEpoch 7/25, Loss: 0.1105\nEpoch 8/25, Loss: 0.0818\nEpoch 9/25, Loss: 0.0705\nEpoch 10/25, Loss: 0.0568\nEpoch 11/25, Loss: 0.0390\nEpoch 12/25, Loss: 0.0417\nEpoch 13/25, Loss: 0.0235\nEpoch 14/25, Loss: 0.0466\nEpoch 15/25, Loss: 0.0198\nEpoch 16/25, Loss: 0.0128\nEpoch 17/25, Loss: 0.0109\nEpoch 18/25, Loss: 0.0104\nEpoch 19/25, Loss: 0.0082\nEpoch 20/25, Loss: 0.0081\nEpoch 21/25, Loss: 0.0867\nEpoch 22/25, Loss: 0.0168\nEpoch 23/25, Loss: 0.0077\nEpoch 24/25, Loss: 0.0062\nEpoch 25/25, Loss: 0.0768\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \naccuracy = correct / total\nprint(f'Accuracy on test set: {accuracy:.2%}')","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:08:09.670858Z","iopub.execute_input":"2024-07-10T15:08:09.671696Z","iopub.status.idle":"2024-07-10T15:08:10.840943Z","shell.execute_reply.started":"2024-07-10T15:08:09.671657Z","shell.execute_reply":"2024-07-10T15:08:10.839602Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Accuracy on test set: 86.56%\n","output_type":"stream"}]}]}